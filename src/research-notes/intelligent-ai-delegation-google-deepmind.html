<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Intelligent AI Delegation (Google DeepMind 2026) - Research Note by David Dasa">
  <title>The Delegation Problem: What Google DeepMind's Framework Means for Clinical AI - Research Notes - David Dasa</title>
  <link rel="icon" href="/src/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="../styles.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/particles.js/2.0.0/particles.min.js"></script>
</head>
<body>

  <nav>
    <ul>
      <li class="logo"><a href="https://www.daviddasa.com"><img src="https://github.com/Naijaoracle/daviddasa/blob/9556670c224c56b73e2a7f21ce1a4e27cbc1a90e/src/DD_logo.png?raw=true" alt="Logo" width="50" height="50"></a></li>
      <li class="home-link"><a href="https://www.daviddasa.com/">Home</a></li>
      <li><a href="https://www.daviddasa.com/about">About</a></li>
      <li><a href="https://www.daviddasa.com/projects">Projects</a></li>
      <li><a href="https://www.daviddasa.com/skills">Skills</a></li>
      <li><a href="https://www.daviddasa.com/research-notes">Research Notes</a></li>
      <li><a href="https://www.daviddasa.com/contact">Contact</a></li>
    </ul>
  </nav>

  <!-- Particles Background -->
  <div id="particles-js"></div>

  <!-- Main Content -->
  <div class="container">
    <main>
      <!-- Back Navigation -->
      <div style="margin-bottom: 2rem;">
        <a href="../research-notes/" style="color: #2ecc71; text-decoration: none; font-weight: 500;">← Back to Research Notes</a>
      </div>

      <!-- Article Header -->
      <section class="project-card">
        <div class="article-header">
          <h1>The Delegation Problem: What Google DeepMind's Framework Means for Clinical AI</h1>
          <div class="article-meta">
            <span class="article-date">February 25, 2026</span>
            <span class="article-type">Paper</span>
          </div>
          <div class="article-tags">
            <span class="tag">AI Agents</span>
            <span class="tag">Multi-Agent Systems</span>
            <span class="tag">AI Safety</span>
            <span class="tag">Healthcare AI</span>
            <span class="tag">Clinical Workflow</span>
          </div>
        </div>
      </section>

      <!-- TLDR -->
      <section class="project-card">
        <h2>TLDR</h2>
        <p>Google DeepMind researchers Tomašev, Franklin, and Osindero propose a framework for <em>intelligent AI delegation</em> — going far beyond simple task decomposition to encompass the transfer of authority, responsibility, trust, and accountability between agents (human or AI). The paper is grounded in decades of organisational theory, pulls directly from aviation and medicine for its core concepts, and proposes five design requirements for safe delegation at scale. For healthcare, the ideas are immediately applicable: every ward round, every handover, every referral is already a delegation event, and the framework gives us precise language for the failures that occur when AI enters those workflows without adequate guardrails.</p>
      </section>

      <!-- Clinical Hook -->
      <section class="project-card">
        <h2>Delegation Is Already What We Do</h2>

        <p>A consultant doing a ward round on 22 patients does not personally perform every action generated by that round. She delegates: to the registrar to chase radiology, to the pharmacist to reconcile the drug chart, to the junior doctor to insert the cannula and re-examine bed 14 in three hours. Each of those delegations involves an implicit contract — a transfer of not just the task but of authority to act, responsibility for the outcome, and an expectation of feedback. When a delegation goes wrong in medicine, it is rarely because the task was too hard. It is because the contract was unclear, the delegatee's capability was misjudged, the oversight loop was absent, or the delegatee had no mechanism to push back when something felt wrong.</p>

        <p>Now picture that consultant's workflow augmented with AI. An LLM agent reads overnight notes and proposes a prioritised review list. A second AI agent drafts discharge summaries for three medically stable patients. A third monitors the deterioration scores and flags anyone whose NEWS has changed since the last review. Each of these is a delegation — and each carries exactly the same risks as the human version, plus several new ones.</p>

        <p>This February 2026 paper from Google DeepMind is the first framework I have read that takes delegation seriously enough to be directly useful in thinking about these scenarios. It does not treat delegation as mere task routing. It treats it as a governance problem.</p>
      </section>

      <!-- The Framework -->
      <section class="project-card">
        <h2>What "Intelligent Delegation" Actually Means</h2>

        <p>The authors define intelligent delegation as a sequence of decisions involving task allocation that also incorporates transfer of authority, responsibility, and accountability; clear specifications regarding roles and boundaries; clarity of intent; and mechanisms for establishing trust between the parties. The emphasis on <em>transfer</em> rather than mere <em>assignment</em> is the key move. When you assign a task you retain ownership. When you delegate it you transfer a bounded slice of authority. The delegatee is not just executing instructions — they are, within the defined scope, acting on behalf of the delegator.</p>

        <p>This distinction matters enormously in clinical contexts. When a hospital deploys an AI triage tool, is it assigning the triage function to the algorithm (retaining full ownership and responsibility) or delegating it (transferring operational authority with the algorithm now accountable for its outputs within a defined scope)? Current deployments rarely answer this question explicitly. The legal and governance consequences of not answering it are significant — a point the EU AI Act and FDA CDS guidance both circle around without quite resolving.</p>

        <h3>Eleven Dimensions of Task Characterisation</h3>
        <p>Before any delegation can be well-designed, the paper argues, the task must be characterised across eleven dimensions. Reading these as a clinician is immediately productive:</p>

        <ul style="line-height: 2;">
          <li><strong>Criticality</strong> — the severity of consequences if performance is suboptimal. Prescribing insulin is high-criticality. Scheduling a routine follow-up is low-criticality. The appropriate level of oversight scales with this.</li>
          <li><strong>Reversibility</strong> — whether the task's effects can be undone. Irreversible actions (administering a drug, discharging a patient, sending a referral) require what the paper calls <em>liability firebreaks</em> — hard checkpoints before execution. Reversible actions (drafting a letter, generating a summary) can be delegated with lighter oversight.</li>
          <li><strong>Verifiability</strong> — how easily the output can be checked. A completed blood result is highly verifiable. "Assess this patient's social circumstances" is low-verifiability and requires either a high-trust delegatee or expensive human review. The paper proposes "contract-first decomposition" as a safety constraint: <em>only delegate what can be verified</em>. If the output is too subjective to check, break the task down further until verifiable sub-tasks emerge.</li>
          <li><strong>Uncertainty</strong> — the ambiguity about inputs, environment, and outcome probability. Clinical environments are structurally high-uncertainty; AI systems that treat uncertainty as a bug rather than a design variable will fail in them.</li>
          <li><strong>Contextuality</strong> — the volume and sensitivity of external state required to execute well. High-contextuality tasks introduce larger privacy surface areas and are harder to safely compartmentalise. Delegating a complex patient's management to an AI requires that AI to hold a lot of sensitive context — raising data governance questions the framework explicitly flags.</li>
        </ul>

        <p>The remaining dimensions — complexity, duration, cost, resource requirements, constraints, and subjectivity — are equally relevant for clinical workflow design, but criticality, reversibility, and verifiability are the three that most directly map to patient safety concerns.</p>
      </section>

      <!-- Human Org Insights -->
      <section class="project-card">
        <h2>What Medicine Already Knows About Delegation Failure</h2>

        <p>One of the most valuable sections of this paper is its grounding in human organisational theory — and the extent to which clinical medicine has already surfaced the exact failure modes the authors are trying to prevent in AI systems.</p>

        <h3>The Authority Gradient</h3>
        <p>Originally coined in aviation, the <em>authority gradient</em> describes how significant disparities in capability, experience, and authority impede communication and lead to errors. The paper notes this has been studied extensively in medicine, where a significant percentage of errors is attributed to the way senior practitioners supervise juniors. A high authority gradient prevents less experienced workers from voicing concerns about a request — even when those concerns are clinically valid.</p>

        <p>In AI delegation the same dynamic applies in both directions. A more capable AI delegator may presume capability in its delegatee and issue requests of inappropriate complexity. A delegatee agent may — due to sycophancy and instruction-following bias — be reluctant to challenge or reject a request, irrespective of whether the request was issued by an AI or a human. We have built AI systems that are, structurally, very good junior doctors: technically capable, broadly compliant, and deeply reluctant to push back on authority. That is not a design virtue.</p>

        <h3>The Zone of Indifference</h3>
        <p>When an authority is accepted, the delegatee develops a <em>zone of indifference</em> — a range of instructions executed without critical deliberation or moral scrutiny. In current AI systems this zone is defined by safety filters and system instructions: as long as a request does not trigger a hard violation, the model complies. This static compliance creates a significant systemic risk as delegation chains lengthen (A → B → C). A broad zone of indifference allows subtle intent mismatches or context-dependent harms to propagate rapidly downstream, with each agent acting as an unthinking router rather than a responsible actor.</p>

        <p>The paper's response to this is one of its most important concepts: <strong>dynamic cognitive friction</strong>. Agents must be capable of recognising when a request, while technically "safe," is contextually ambiguous enough to warrant stepping <em>outside</em> their zone of indifference to challenge the delegator or request human verification. In clinical terms, this is the difference between a nurse who administers every prescribed medication because none triggered a hard allergy alert, and a nurse who pauses when the dose seems unusual for the patient's renal function even though no hard rule was violated. Dynamic cognitive friction is the AI version of that second nurse.</p>

        <h3>The Principal-Agent Problem in Healthcare AI</h3>
        <p>Classical economics has long studied what happens when a principal delegates to an agent whose motivations are not perfectly aligned. The agent may prioritise their own interests, withhold information, and act in ways that compromise the principal's intent. For AI, the authors note this manifests as reward misspecification and specification gaming — optimising for the stated metric in ways that subvert the actual goal. The clinical version of this is already familiar: an AI optimised for length-of-stay reduction that systematically flags patients for early discharge regardless of clinical readiness; a triage algorithm optimised for throughput that deprioritises complex patients who will consume disproportionate time. The principal (the clinical team) specified efficiency. The agent found a route to measured efficiency that diverged from intended care quality.</p>
      </section>

      <!-- The Five Requirements -->
      <section class="project-card">
        <h2>Five Requirements for Safe Delegation at Scale</h2>

        <p>The framework's centrepiece is five requirements that any intelligent delegation system must satisfy. They map onto clinical governance concerns with uncomfortable precision.</p>

        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
          <div style="background: rgba(46, 204, 113, 0.08); padding: 1.25rem; border-radius: 0.5rem; border-left: 4px solid rgba(46,204,113,0.4);">
            <h4 style="margin: 0 0 0.5rem; color: #092917;">1. Dynamic Assessment</h4>
            <p style="margin: 0; color: #4a4a4a; font-size: 0.9rem;">Continuous inference of the delegatee's current competence, reliability, and capacity — not a one-time check at onboarding. For clinical AI, this means runtime performance monitoring, not just pre-deployment validation.</p>
          </div>
          <div style="background: rgba(46, 204, 113, 0.08); padding: 1.25rem; border-radius: 0.5rem; border-left: 4px solid rgba(46,204,113,0.4);">
            <h4 style="margin: 0 0 0.5rem; color: #092917;">2. Adaptive Execution</h4>
            <p style="margin: 0; color: #4a4a4a; font-size: 0.9rem;">Delegators must retain the ability to switch delegatees mid-task when performance degrades or circumstances change. A clinical AI workflow must be interruptible — not locked into a path once initiated.</p>
          </div>
          <div style="background: rgba(46, 204, 113, 0.08); padding: 1.25rem; border-radius: 0.5rem; border-left: 4px solid rgba(46,204,113,0.4);">
            <h4 style="margin: 0 0 0.5rem; color: #092917;">3. Structural Transparency</h4>
            <p style="margin: 0; color: #4a4a4a; font-size: 0.9rem;">Auditability of both process and outcome. Not just what decision was made, but by which agent, on what basis, with what authority. In clinical AI, opacity between incompetence and malice is unacceptable.</p>
          </div>
          <div style="background: rgba(46, 204, 113, 0.08); padding: 1.25rem; border-radius: 0.5rem; border-left: 4px solid rgba(46,204,113,0.4);">
            <h4 style="margin: 0 0 0.5rem; color: #092917;">4. Scalable Market Coordination</h4>
            <p style="margin: 0; color: #4a4a4a; font-size: 0.9rem;">Efficient, trusted mechanisms for matching tasks to capable delegatees at scale — relevant as AI agent ecosystems in healthcare grow to include specialist diagnostic models, documentation agents, and scheduling systems.</p>
          </div>
          <div style="background: rgba(46, 204, 113, 0.08); padding: 1.25rem; border-radius: 0.5rem; border-left: 4px solid rgba(46,204,113,0.4);">
            <h4 style="margin: 0 0 0.5rem; color: #092917;">5. Systemic Resilience</h4>
            <p style="margin: 0; color: #4a4a4a; font-size: 0.9rem;">Preventing cascading failures across the delegation network. Hyper-efficient designs without redundancy create brittle architectures where a single point of failure propagates. Healthcare systems have learned this lesson the hard way.</p>
          </div>
        </div>

        <p>The paper also introduces "contract-first decomposition" as a binding constraint: task delegation must be contingent on the outcome having a precise verification mechanism. If a sub-task's output is too subjective, costly, or complex to verify, the system should decompose it further until verifiable units emerge. This is a direct counterpart to the FDA's Criterion 4 and the EU AI Act's human oversight requirement — arriving at the same place from a different direction. You cannot oversee what you cannot verify.</p>
      </section>

      <!-- Conclusion -->
      <section class="project-card">
        <h2>Conclusion</h2>

        <p>What I find most valuable about this paper is that it gives healthcare AI the vocabulary it has been missing. We have had "human-in-the-loop" as a slogan for years without a precise account of what the loop should contain, how handoffs should be structured, when the human should be invoked, and who bears responsibility when the loop fails. The intelligent delegation framework answers those questions systematically.</p>

        <p>The concepts that land hardest for me clinically are the authority gradient — which explains why AI systems that are structurally deferential are not safe just because they are compliant — and dynamic cognitive friction, which is the design principle that distinguishes a safe AI collaborator from a sophisticated yes-machine. Building friction into AI clinical tools is counterintuitive; it feels like adding latency and reducing efficiency. But frictionless compliance in a high-stakes delegation chain is not efficiency. It is a latent failure waiting for the right circumstances to become an incident.</p>

        <p>The paper is a preprint from February 2026 and does not yet contain empirical validation of the framework. But as a conceptual grounding for what multi-agent clinical AI should look like — and for what questions procurement, governance, and clinical safety teams should be asking when evaluating these tools — it is one of the most practically useful papers I have read in this space.</p>

        <h3>References</h3>
        <p>Original paper: Tomašev, N., Franklin, M., &amp; Osindero, S. (2026). <em>Intelligent AI Delegation</em>. Google DeepMind. arXiv:2602.11865v1.</p>
      </section>

    </main>
  </div>

  <!-- Footer Section -->
  <footer>
    <p>&copy; 2026 David Dasa</p>
  </footer>

  <!-- Particles.js Configuration -->
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      particlesJS('particles-js', {
        particles: {
          number: { value: 80, density: { enable: true, value_area: 800 } },
          color: { value: '#2ecc71' },
          shape: { type: 'circle' },
          opacity: { value: 0.5, random: false },
          size: { value: 3, random: true },
          line_linked: {
            enable: true,
            distance: 150,
            color: '#2ecc71',
            opacity: 0.4,
            width: 1
          },
          move: {
            enable: true,
            speed: 2,
            direction: 'none',
            random: false,
            straight: false,
            out_mode: 'out',
            bounce: false,
          }
        },
        interactivity: {
          detect_on: 'canvas',
          events: {
            onhover: { enable: true, mode: 'repulse' },
            onclick: { enable: true, mode: 'push' },
            resize: true
          }
        },
        retina_detect: true
      });
    });
  </script>

  <style>
    .article-header h1 { color: #092917; margin-bottom: 1rem; font-size: 2.2rem; }
    .article-meta { display: flex; gap: 2rem; margin-bottom: 1rem; flex-wrap: wrap; }
    .article-date { color: #2ecc71; font-weight: 600; font-size: 1rem; }
    .article-type { color: #1a4731; font-weight: 500; background: rgba(46, 204, 113, 0.1); padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.9rem; }
    .article-tags { display: flex; gap: 0.5rem; flex-wrap: wrap; }
    .tag { background: rgba(46, 204, 113, 0.12); color: #092917; padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.8rem; font-weight: 500; }
    .container { max-width: 1000px; }
    main { line-height: 1.7; }
    p { line-height: 1.7; }
    ul { line-height: 1.7; padding-left: 1.5rem; }
    li { margin-bottom: 0.4rem; }
    .project-card { background: rgba(255,255,255,0.82); border: 1px solid rgba(0,0,0,0.06); box-shadow: 0 6px 18px rgba(0,0,0,0.04); }
    @media (max-width: 768px) {
      .article-header h1 { font-size: 1.8rem; }
      .article-meta { flex-direction: column; gap: 0.5rem; }
    }
  </style>

</body>
</html>
