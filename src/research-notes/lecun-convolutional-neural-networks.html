<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Convolutional Neural Networks: The Architecture That Learned to Read - Research Note by David Dasa">
  <title>Convolutional Neural Networks: The Architecture That Learned to Read - Research Notes - David Dasa</title>
  <link rel="icon" href="/src/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="../styles.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/particles.js/2.0.0/particles.min.js"></script>
</head>
<body>
  <nav>
    <ul>
      <li class="logo"><a href="https://www.daviddasa.com"><img src="https://github.com/Naijaoracle/daviddasa/blob/9556670c224c56b73e2a7f21ce1a4e27cbc1a90e/src/DD_logo.png?raw=true" alt="Logo" width="50" height="50"></a></li>
      <li class="home-link"><a href="https://www.daviddasa.com/">Home</a></li>
      <li><a href="https://www.daviddasa.com/research-notes">Research Notes</a></li>
    </ul>
  </nav>

  <!-- Particles Background -->
  <div id="particles-js"></div>

  <div class="container">
    <main>
      <!-- Back Navigation -->
      <div style="margin-bottom: 2rem;">
        <a href="../research-notes/" style="color: #2ecc71; text-decoration: none; font-weight: 500;">← Back to Research Notes</a>
      </div>
      <!-- INTRO -->
      <section class="project-card">
        <div class="article-header">
          <h1>Convolutional Neural Networks: The Architecture That Learned to Read</h1>
          <div class="article-meta">
            <span class="article-date">November 1, 2025</span>
            <span class="article-type">Paper</span>
          </div>
          <div class="article-tags">
            <span class="tag">AI/ML</span>
            <span class="tag">Computer Vision</span>
            <span class="tag">Healthcare AI</span>
            <span class="tag">Medical Imaging</span>
          </div>
        </div>
        <h2>INTRO</h2>
        <p>In 1998, Yann LeCun and colleagues published a long, careful field report on how to get machines to read—postal codes, bank checks, full words—without hand-crafting fragile rules. The paper's plain thesis: if you pick the right architecture and train it end-to-end with gradients, the network can learn its own features directly from pixels and beat traditional pipelines. That architecture is the convolutional neural network (CNN), and the exemplar is LeNet. The authors also introduce a way to train entire, multi-module recognition systems together—Graph Transformer Networks (GTNs)—and show the approach at industrial scale by reading millions of bank checks per day. For healthcare folks, this isn't just history. It's the blueprint for modern imaging AI, ECG analysis, pathology slide classification, and any workflow where raw signals must become reliable, low-latency decisions.</p>
      </section>

      <!-- TLDR -->
      <section class="project-card">
        <h2>TLDR</h2>
        <p>CNNs—built from small, repeated filters with shared weights and occasional down-sampling—learn translation-tolerant features and need far fewer parameters than fully connected nets. Trained by backpropagation, they outperform rival methods on handwritten digit benchmarks and scale into production. The same ingredients (end-to-end learning, weight sharing, light preprocessing, and global training of multi-step systems) map cleanly onto clinical tasks from OCR in EHRs to radiology triage.</p>
      </section>

      <!-- CONTENT -->
      <section class="project-card">
        <h2>CONTENT</h2>
        <p>The story starts with a practical observation: brittle, hand-tuned features don't generalize. If you let a network see the raw image and push the error signal all the way to the first pixels, it will discover features that matter for the task. That's the "gradient-based learning" promise, and the paper contrasts it with methods that bolt a classifier atop human-designed feature extractors.</p>
        
        <p>What makes a CNN special? Three ideas that healthcare readers will recognize from today's models, each addressing fundamental challenges in medical image analysis.</p>

        <p>The first breakthrough is local receptive fields with shared weights. Instead of learning a separate weight for every pixel-to-neuron connection, a small filter slides across the image, using the same few numbers at every location. This "weight sharing" dramatically reduces trainable parameters and builds in translation tolerance—vital when the same pathological structure appears in different places on a chest X-ray or histology tile. LeNet-5 had hundreds of thousands of connections but only tens of thousands of free parameters because many connections reused the same filter weights.</p>

        <p>The second innovation involves alternating convolution and subsampling layers. As you go deeper, spatial resolution shrinks while the number of feature maps grows. You lose exact pixel locations but gain richer, more abstract representations—think "there's a nodule-like edge pattern somewhere in this region" rather than "a bright pixel at coordinates (x,y)." The paper links this design to simple and complex cells in visual cortex, but the engineering benefit is robust invariance to small shifts and deformations that plague medical imaging.</p>

        <p>The third pillar is end-to-end training with backpropagation. Gradients flow from the final decision through all layers back to the raw pixels. Contrary to earlier concerns about local minima, the authors demonstrate that backpropagation works well in practice, especially when networks have sufficient capacity. They also highlight the power of stochastic updates over heavy second-order optimizers for large datasets—a lesson that modern healthcare AI systems still follow religiously.</p>

        <p>Do these design choices pay off? Absolutely. On handwritten digit recognition, CNNs outperformed the alternative approaches of the time—nearest neighbors, tangent distance, and support vector machines at comparable memory and computation costs. Boosting a small committee of CNNs squeezed accuracy even further. The paper is refreshingly honest about trade-offs, noting that SVMs can be very accurate but costly in memory and computation, yet the through-line remains clear: with enough data and the right inductive bias, CNNs are remarkably hard to beat.</p>

        <p>The paper then widens its lens from single characters to complete documents, revealing insights that directly apply to modern healthcare AI systems. Real-world systems have multiple moving parts—finding fields, segmenting characters, recognizing them, and applying language constraints. Train those modules separately and you get local optima at the system level; train them together and you can minimize the error you actually care about, like the read rate for an entire medical form. Their Graph Transformer Networks provide a mechanism for "global training" over directed graphs of modules, with tangible payoff: a deployed system reading several million bank checks per day. That production claim matters—this wasn't academic toy-making but engineered reality.</p>

        <p>The healthcare translation is remarkably straightforward. In medical imaging, CNNs' locality and weight sharing match the fundamental physics of how images are formed. They learn edges, textures, and shapes that discriminate pleural effusions from normal lung fields, tumors from healthy tissue, or hemorrhages from normal brain parenchyma—all without brittle hand-crafted features. The invariances built through subsampling layers mirror what makes a model tolerate different scanner positions or small patient movements during acquisition.</p>

        <p>For clinical waveforms, time-delay neural networks—essentially 1-D CNN cousins—apply the same architectural recipe along the time dimension. This approach proves invaluable for ECG rhythm classification or respiratory waveform analysis in critical care settings. The paper explicitly connects CNNs and TDNNs for phoneme recognition; swap audio signals for ECG traces, and the mapping becomes obvious.</p>

        <p>The check-reading demonstration provides a direct blueprint for EHR digitization challenges. Claims forms, lab requisitions, and legacy scanned charts all benefit from the same approach: use a CNN for character and word recognition, wire it into a graph that models field structure and language constraints, then train the entire pipeline toward the end goal—correct patient ID plus correct test code—rather than optimizing just per-character accuracy.</p>

        <p>Along the way, the authors document practical tricks that still show up in hospital-grade AI: data augmentation via small geometric distortions; committee/ensemble methods (boosting) to reduce residual error; and careful accounting of memory and runtime so models fit inside operational constraints.</p>
      </section>

      <!-- CONCLUSION -->
      <section class="project-card">
        <h2>CONCLUSION</h2>
        <p>For clinicians, buyers, and regulators, the 1998 paper remains valuable not for nostalgia but because it establishes durable engineering principles for AI systems that touch real clinical workflows. The authors demonstrate that we should prefer architectures that embed problem structure—locality and invariance—over clever post-hoc feature engineering. They show the importance of training end-to-end on the outcome you actually care about rather than relying on intermediate proxies that may not align with clinical goals.</p>

        <p>Crucially, they keep a careful eye on memory and computational requirements, not just accuracy metrics. Clinical deployment operates within real budgets and infrastructure constraints. The paper also demonstrates that steady gains come from simple, well-understood tools like data augmentation and ensemble methods rather than exotic algorithms that are difficult to validate and deploy safely.</p>

        <p>Modern radiology and pathology CNNs are orders of magnitude larger and trained on powerful GPUs, but philosophically they remain LeNet's children. The fundamental insights about local receptive fields, weight sharing, and end-to-end learning continue to drive the systems that help diagnose cancer, detect fractures, and identify pathological changes in medical images. If you want a single historical paper that still informs how to build safe, effective clinical perception systems today, this is it.</p>
        <h3>References</h3>
        <p>Original paper: <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a>.</p>
      </section>
    </main>
  </div>

  <!-- Particles.js Configuration to match Turing page -->
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      particlesJS('particles-js', {
        particles: {
          number: { value: 80, density: { enable: true, value_area: 800 } },
          color: { value: '#2ecc71' },
          shape: { type: 'circle' },
          opacity: { value: 0.5, random: false },
          size: { value: 3, random: true },
          line_linked: {
            enable: true,
            distance: 150,
            color: '#2ecc71',
            opacity: 0.4,
            width: 1
          },
          move: {
            enable: true,
            speed: 2,
            direction: 'none',
            random: false,
            straight: false,
            out_mode: 'out',
            bounce: false,
          }
        },
        interactivity: {
          detect_on: 'canvas',
          events: {
            onhover: { enable: true, mode: 'repulse' },
            onclick: { enable: true, mode: 'push' },
            resize: true
          }
        },
        retina_detect: true
      });
    });
  </script>

  <style>
    /* Copy layout refinements used on Turing page */
    .article-header h1 { color: #092917; margin-bottom: 1rem; font-size: 2.2rem; }
    .article-meta { display: flex; gap: 2rem; margin-bottom: 1rem; flex-wrap: wrap; }
    .article-date { color: #2ecc71; font-weight: 600; font-size: 1rem; }
    .article-type { color: #1a4731; font-weight: 500; background: rgba(46, 204, 113, 0.1); padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.9rem; }
    .article-tags { display: flex; gap: 0.5rem; flex-wrap: wrap; }
    .tag { background: rgba(46, 204, 113, 0.12); color: #092917; padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.8rem; font-weight: 500; }
    .container { max-width: 1000px; }
    main { line-height: 1.7; }
    p { line-height: 1.7; }
    .project-card { background: rgba(255,255,255,0.82); border: 1px solid rgba(0,0,0,0.06); box-shadow: 0 6px 18px rgba(0,0,0,0.04); }
    @media (max-width: 768px) {
      .article-header h1 { font-size: 1.8rem; }
      .article-meta { flex-direction: column; gap: 0.5rem; }
    }
  </style>
</body>
</html>
