<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Benchmarking Principles - Research Artefact by David Dasa">
  <title>Benchmarking Principles - Research Artefacts - David Dasa</title>
  <link rel="icon" href="/src/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="../../styles.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/particles.js/2.0.0/particles.min.js"></script>
</head>
<body>

    <nav>
    <ul>
      <li class="logo"><a href="https://www.daviddasa.com"><img src="https://github.com/Naijaoracle/daviddasa/blob/9556670c224c56b73e2a7f21ce1a4e27cbc1a90e/src/DD_logo.png?raw=true" alt="Logo" width="50" height="50"></a></li>
      <li class="home-link"><a href="https://www.daviddasa.com/">Home</a></li>
      <li><a href="https://www.daviddasa.com/about">About</a></li>
      <li><a href="https://www.daviddasa.com/projects">Projects</a></li>
      <li><a href="https://www.daviddasa.com/skills">Skills</a></li>
      <li><a href="https://www.daviddasa.com/research-notes">Research Notes</a></li>
      <li><a href="https://www.daviddasa.com/research-artefacts">Research Artefacts</a></li>
      <li><a href="https://www.daviddasa.com/contact">Contact</a></li>
    </ul>
  </nav>

  <!-- Particles Background -->
  <div id="particles-js"></div>

  <!-- Main Content -->
  <div class="container">
    <main>
      <!-- Back Navigation -->
      <div style="margin-bottom: 2rem;">
        <a href="../index.html" style="color: #2ecc71; text-decoration: none; font-weight: 500;">← Back to Research Artefacts</a>
      </div>

      <!-- Article Header -->
      <section class="project-card">
        <div class="article-header">
          <h1>Benchmarking Principles</h1>
          <div class="article-meta">
            <span class="article-category">Benchmarking</span>
            <span class="article-type">Research Artefact</span>
          </div>
          <div class="article-tags">
            <span class="tag">Benchmarking</span>
            <span class="tag">Research</span>
            <span class="tag">Methodology</span>
          </div>
        </div>

        <h2>Purpose and Philosophy</h2>
        <p>This document outlines principles for designing benchmark suites using synthetic scenarios to evaluate AI-driven NPCs in XR healthcare simulations. Benchmarking in this context serves a fundamentally different purpose than traditional machine learning benchmarks — we are not optimising for a single performance metric, but assessing <strong>multidimensional fitness for educational purpose</strong>.</p>

        <p>The healthcare AI simulation domain presents unique benchmarking challenges:</p>
        <ul>
          <li><strong>No ground truth:</strong> Unlike diagnostic AI, there is no single "correct" NPC behaviour — appropriateness depends on educational context, learner level, and scenario objectives.</li>
          <li><strong>Qualitative outcomes:</strong> Success is measured in learning effectiveness and clinical plausibility, not accuracy percentages.</li>
          <li><strong>Context sensitivity:</strong> The same NPC behaviour may be appropriate in one scenario and inappropriate in another.</li>
          <li><strong>Multistakeholder evaluation:</strong> Validity requires input from clinical experts, educators, simulation specialists, and learners.</li>
        </ul>

        <div class="principle-callout">
          <h4>Guiding Philosophy</h4>
          <p>Benchmarks should reveal <em>how</em> systems behave across diverse contexts, not just <em>whether</em> they pass or fail. The goal is characterisation, not certification. A well-designed benchmark illuminates strengths and weaknesses that guide iterative improvement — it does not produce a single "better/worse" ranking.</p>
        </div>

        <h3>Scope Boundaries</h3>
        <ul>
          <li>No benchmark packs, automated runners, or executable test suites are provided.</li>
          <li>Benchmarks are descriptive frameworks intended for human-led evaluation.</li>
          <li>Results apply only to synthetic, non-clinical research contexts.</li>
          <li>Benchmark results do not predict clinical performance or real-world safety.</li>
        </ul>
      </section>

      <!-- Core Principles -->
      <section class="project-card">
        <h2>Core Benchmarking Principles</h2>
        <p>The following principles guide benchmark design for XR-NPC evaluation. Each principle addresses a specific challenge in creating meaningful, reproducible assessments.</p>

        <div class="principle-grid">
          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">1</span>
              <h3>Coverage</h3>
            </div>
            <p>Benchmark suites must include diverse clinical contexts and decision types to reveal system behaviour across the intended operational envelope.</p>
            
            <h4>Coverage Dimensions</h4>
            <table>
              <thead>
                <tr><th>Dimension</th><th>Coverage Targets</th><th>Rationale</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td>Clinical specialty</td>
                  <td>Emergency, acute care, primary care, specialty referral</td>
                  <td>Different specialties have distinct reasoning patterns and workflow constraints.</td>
                </tr>
                <tr>
                  <td>Acuity level</td>
                  <td>Stable, deteriorating, critical, end-of-life</td>
                  <td>NPC behaviour should adapt appropriately to patient acuity.</td>
                </tr>
                <tr>
                  <td>Decision type</td>
                  <td>Diagnostic, therapeutic, prognostic, procedural, handoff</td>
                  <td>Different decision types require different reasoning and communication patterns.</td>
                </tr>
                <tr>
                  <td>Uncertainty level</td>
                  <td>Clear presentation, ambiguous, contradictory, incomplete data</td>
                  <td>Appropriate uncertainty handling is critical for safety assessment.</td>
                </tr>
                <tr>
                  <td>Team composition</td>
                  <td>Solo practitioner, dyad, full team, cross-disciplinary</td>
                  <td>Team dynamics affect communication patterns and role expectations.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">2</span>
              <h3>Variation</h3>
            </div>
            <p>Benchmarks must systematically vary scenario parameters to distinguish robust behaviours from context-dependent artefacts.</p>
            
            <h4>Variation Strategy</h4>
            <table>
              <thead>
                <tr><th>Variation Type</th><th>Parameters</th><th>Purpose</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td>Stressor variation</td>
                  <td>Time pressure, resource constraints, competing priorities, distractions</td>
                  <td>Assess performance degradation under realistic clinical stress.</td>
                </tr>
                <tr>
                  <td>Information variation</td>
                  <td>Data availability, quality, timing, source reliability</td>
                  <td>Test appropriate handling of incomplete or uncertain information.</td>
                </tr>
                <tr>
                  <td>Narrative variation</td>
                  <td>Patient demographics, presentation style, communication ability, social context</td>
                  <td>Detect potential biases and ensure equitable performance.</td>
                </tr>
                <tr>
                  <td>Interaction variation</td>
                  <td>Learner expertise level, question types, challenge intensity</td>
                  <td>Assess adaptive behaviour across learner capabilities.</td>
                </tr>
              </tbody>
            </table>

            <p style="margin-top: 1rem;"><strong>Variation principle:</strong> For each core scenario, create at least 3 systematic variations. Document which parameters are varied and the expected impact on NPC behaviour.</p>
          </div>

          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">3</span>
              <h3>Transparency</h3>
            </div>
            <p>All benchmark assumptions, limitations, and design decisions must be explicitly documented to enable appropriate interpretation and replication.</p>
            
            <h4>Transparency Requirements</h4>
            <ul>
              <li><strong>Assumption documentation:</strong> Every scenario must reference the relevant entries in the <a href="../digital-twin/physiological-assumptions.html">Physiological Assumptions</a> and <a href="../digital-twin/workflow-assumptions.html">Workflow Assumptions</a> registers.</li>
              <li><strong>Synthetic boundary markers:</strong> All benchmark materials must be clearly labelled as synthetic and non-clinical.</li>
              <li><strong>Design rationale:</strong> Document why each scenario was included, what it is intended to test, and what constitutes success or failure.</li>
              <li><strong>Limitation acknowledgment:</strong> Explicitly state what the benchmark does not assess and what conclusions cannot be drawn from results.</li>
              <li><strong>Expert provenance:</strong> Document which clinical experts contributed to scenario design and validation.</li>
            </ul>
          </div>

          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">4</span>
              <h3>Comparability</h3>
            </div>
            <p>Benchmarks must use consistent formats and evaluation criteria to enable meaningful comparison across systems, versions, or evaluation sessions.</p>
            
            <h4>Comparability Standards</h4>
            <table>
              <thead>
                <tr><th>Element</th><th>Standardisation Requirement</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scenario format</td>
                  <td>Use the <a href="../scenario-management/scenario-template-guide.html">Scenario Template</a> consistently across all benchmark scenarios.</td>
                </tr>
                <tr>
                  <td>Evaluation rubrics</td>
                  <td>Apply the <a href="../evaluation/metric-catalogue.html">Metric Catalogue</a> rubrics with documented anchor interpretations.</td>
                </tr>
                <tr>
                  <td>Reporting structure</td>
                  <td>Use the <a href="../evaluation/reporting-template.html">Reporting Template</a> for all benchmark results.</td>
                </tr>
                <tr>
                  <td>Evaluator calibration</td>
                  <td>Conduct inter-rater reliability checks; document agreement levels.</td>
                </tr>
                <tr>
                  <td>Version control</td>
                  <td>Record exact scenario versions, evaluation dates, and any protocol deviations.</td>
                </tr>
              </tbody>
            </table>
          </div>

          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">5</span>
              <h3>Progressivity</h3>
            </div>
            <p>Benchmark suites should include scenarios of graduated difficulty to characterise system capabilities across complexity levels.</p>
            
            <h4>Complexity Tiers</h4>
            <table>
              <thead>
                <tr><th>Tier</th><th>Characteristics</th><th>Purpose</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Foundation</strong></td>
                  <td>Clear presentation, single problem, adequate resources, minimal time pressure</td>
                  <td>Establish baseline capability on straightforward cases.</td>
                </tr>
                <tr>
                  <td><strong>Standard</strong></td>
                  <td>Typical complexity, some ambiguity, realistic constraints, normal pace</td>
                  <td>Assess performance on representative clinical scenarios.</td>
                </tr>
                <tr>
                  <td><strong>Complex</strong></td>
                  <td>Multiple problems, significant uncertainty, resource constraints, time pressure</td>
                  <td>Test resilience under challenging conditions.</td>
                </tr>
                <tr>
                  <td><strong>Edge</strong></td>
                  <td>Rare presentations, conflicting information, ethical dilemmas, system boundaries</td>
                  <td>Explore failure modes and boundary behaviours.</td>
                </tr>
              </tbody>
            </table>
            <p style="margin-top: 1rem;"><strong>Balance principle:</strong> A benchmark suite should include approximately 40% Foundation/Standard scenarios and 60% Complex/Edge scenarios to characterise both typical and boundary performance.</p>
          </div>

          <div class="principle-item">
            <div class="principle-header">
              <span class="principle-number">6</span>
              <h3>Educational Alignment</h3>
            </div>
            <p>Benchmarks must be designed with explicit educational objectives, ensuring that evaluation criteria reflect pedagogical value rather than just technical performance.</p>
            
            <h4>Alignment Considerations</h4>
            <ul>
              <li><strong>Learning outcome mapping:</strong> Each benchmark scenario should map to specific learning objectives from recognised clinical competency frameworks.</li>
              <li><strong>Pedagogical appropriateness:</strong> NPC behaviours should be evaluated for their educational value, not just clinical accuracy — sometimes a "teaching moment" requires different behaviour than optimal clinical efficiency.</li>
              <li><strong>Debriefing enablement:</strong> Benchmark scenarios should generate outputs that support meaningful post-simulation debriefing discussions.</li>
              <li><strong>Progression support:</strong> Results should inform decisions about learner progression and targeted skill development.</li>
            </ul>
          </div>
        </div>
      </section>

      <!-- Related Artefacts -->
      <section class="project-card">
        <h2>Related Artefacts</h2>
        <ul>
          <li><a href="reproducibility-checklist.html">Reproducibility Checklist</a> — Requirements for replicable benchmark execution and reporting</li>
          <li><a href="../scenario-management/vignette-library.html">Scenario Vignette Library</a> — Example scenarios for inclusion in benchmark suites</li>
          <li><a href="../evaluation/metric-catalogue.html">Metric Catalogue</a> — Standardised metrics for benchmark evaluation</li>
          <li><a href="../evaluation/reporting-template.html">Reporting Template</a> — Structure for documenting benchmark results</li>
        </ul>
      </section>
</main>
  </div>

  <!-- Particles.js Configuration -->
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      particlesJS('particles-js', {
        particles: {
          number: { value: 80, density: { enable: true, value_area: 800 } },
          color: { value: '#2ecc71' },
          shape: { type: 'circle' },
          opacity: { value: 0.5, random: false },
          size: { value: 3, random: true },
          line_linked: {
            enable: true,
            distance: 150,
            color: '#2ecc71',
            opacity: 0.4,
            width: 1
          },
          move: {
            enable: true,
            speed: 2,
            direction: 'none',
            random: false,
            straight: false,
            out_mode: 'out',
            bounce: false,
          }
        },
        interactivity: {
          detect_on: 'canvas',
          events: {
            onhover: { enable: true, mode: 'repulse' },
            onclick: { enable: true, mode: 'push' },
            resize: true
          }
        },
        retina_detect: true
      });
    });
  </script>

  <style>
    /* Layout refinements for cleaner reading */
    .article-header h1 { color: #092917; margin-bottom: 1rem; font-size: 2.2rem; }
    .article-meta { display: flex; gap: 2rem; margin-bottom: 1rem; flex-wrap: wrap; }
    .article-category { color: #2ecc71; font-weight: 600; font-size: 1rem; }
    .article-type { color: #1a4731; font-weight: 500; background: rgba(46, 204, 113, 0.1); padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.9rem; }
    .article-tags { display: flex; gap: 0.5rem; flex-wrap: wrap; }
    .tag { background: rgba(46, 204, 113, 0.12); color: #092917; padding: 0.25rem 0.75rem; border-radius: 1rem; font-size: 0.8rem; font-weight: 500; }
    .container { max-width: 1000px; }
    main { line-height: 1.7; }
    p { line-height: 1.7; }
    .project-card { background: rgba(255,255,255,0.82); border: 1px solid rgba(0,0,0,0.06); box-shadow: 0 6px 18px rgba(0,0,0,0.04); margin-bottom: 2rem; padding: 2rem; }
    table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
    table th, table td { border: 1px solid rgba(0,0,0,0.1); padding: 0.75rem; text-align: left; }
    table th { background: rgba(46, 204, 113, 0.1); font-weight: 600; color: #092917; }
    ul, ol { margin: 1rem 0; padding-left: 2rem; }
    li { margin: 0.5rem 0; }
    code { background: rgba(46, 204, 113, 0.1); padding: 0.2rem 0.4rem; border-radius: 0.25rem; font-family: 'Courier New', monospace; font-size: 0.9em; }
    pre { background: rgba(46, 204, 113, 0.1); padding: 1rem; border-radius: 0.5rem; overflow-x: auto; }
    pre code { background: none; padding: 0; }
    
    /* Principle Callout */
    .principle-callout {
      background: linear-gradient(135deg, rgba(46, 204, 113, 0.1) 0%, rgba(46, 204, 113, 0.03) 100%);
      border-left: 4px solid #2ecc71;
      padding: 1.25rem 1.5rem;
      border-radius: 0 0.5rem 0.5rem 0;
      margin: 1.5rem 0;
    }
    .principle-callout h4 {
      margin: 0 0 0.5rem 0;
      color: #2ecc71;
    }
    .principle-callout p {
      margin: 0;
      font-style: italic;
    }
    
    /* Principle Grid */
    .principle-grid {
      display: flex;
      flex-direction: column;
      gap: 2rem;
    }
    .principle-item {
      padding: 1.5rem;
      background: rgba(46, 204, 113, 0.03);
      border-radius: 0.75rem;
      border: 1px solid rgba(46, 204, 113, 0.1);
    }
    .principle-header {
      display: flex;
      align-items: center;
      gap: 1rem;
      margin-bottom: 1rem;
    }
    .principle-number {
      display: flex;
      align-items: center;
      justify-content: center;
      width: 2.5rem;
      height: 2.5rem;
      background: linear-gradient(135deg, #2ecc71 0%, #27ae60 100%);
      color: white;
      border-radius: 50%;
      font-weight: 700;
      font-size: 1.1rem;
      flex-shrink: 0;
    }
    .principle-header h3 {
      margin: 0;
      color: #092917;
      font-size: 1.3rem;
    }
    .principle-item h4 {
      color: #1a4731;
      margin-top: 1.5rem;
      margin-bottom: 0.75rem;
      font-size: 1rem;
    }
    
    @media (max-width: 768px) {
      .article-header h1 { font-size: 1.8rem; }
      .article-meta { flex-direction: column; gap: 0.5rem; }
      .project-card { padding: 1.5rem; }
      .principle-item { padding: 1rem; }
      table { font-size: 0.8rem; }
      table th, table td { padding: 0.5rem; }
    }
  </style>

</body>
</html>
