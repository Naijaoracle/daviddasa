<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Machine Learning Decision Boundaries Visualizer">
  <title>ML Boundaries Visualizer - David Dasa</title>
  <link rel="icon" type="image/svg+xml" href="/src/favicon-DD-monogram.svg">
  <link rel="stylesheet" href="styles.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/particles.js/2.0.0/particles.min.js"></script>
  <style>
    .sim-layout {
      display: grid;
      grid-template-columns: 300px 1fr;
      gap: 2rem;
      margin: 2rem auto;
      max-width: 1200px;
    }

    @media (max-width: 900px) {
      .sim-layout {
        grid-template-columns: 1fr;
      }
      .canvas-container {
        height: 400px;
        order: -1; /* Show visualization first on mobile */
      }
    }

    .controls {
      background: rgba(255,255,255,0.82);
      border: 1px solid rgba(0,0,0,0.06);
      border-radius: 1rem;
      padding: 1.5rem;
      box-shadow: 0 6px 18px rgba(0,0,0,0.04);
    }

    .canvas-container {
      background: #f8f9fa;
      border-radius: 1rem;
      overflow: hidden;
      box-shadow: inset 0 0 20px rgba(0,0,0,0.05);
      position: relative;
      height: 600px;
    }

    canvas {
      display: block;
      width: 100%;
      height: 100%;
    }

    .control-group {
      margin-bottom: 1.5rem;
    }

    label {
      display: block;
      font-weight: 600;
      margin-bottom: 0.5rem;
      color: #092917;
      font-size: 0.9rem;
    }

    select, button {
      width: 100%;
      padding: 0.8rem;
      border: 1px solid #ddd;
      border-radius: 0.5rem;
      font-size: 1rem;
      margin-bottom: 0.5rem;
    }

    .btn-primary { background: #2ecc71; color: white; border: none; font-weight: 600; cursor: pointer; }
    .btn-primary:hover { opacity: 0.9; }

    .description {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 1rem;
      line-height: 1.4;
    }

    .legend {
      display: flex;
      gap: 1rem;
      margin-top: 1rem;
      justify-content: center;
    }
    .legend-item { display: flex; align-items: center; gap: 0.5rem; font-size: 0.8rem; }
    .dot { width: 12px; height: 12px; border-radius: 50%; }
  </style>
</head>
<body>
  <nav>
    <ul>
      <li class="logo"><a href="https://www.daviddasa.com"><img src="https://github.com/Naijaoracle/daviddasa/blob/9556670c224c56b73e2a7f21ce1a4e27cbc1a90e/src/DD_logo.png?raw=true" alt="Logo" width="50" height="50"></a></li>
      <li class="home-link"><a href="https://www.daviddasa.com/">Home</a></li>
      <li><a href="https://www.daviddasa.com/projects">Projects</a></li>
    </ul>
  </nav>

  <div id="particles-js"></div>

  <div class="container" style="margin-top: 2rem;">
    <div class="article-header">
      <h1>üß† ML Decision Boundaries</h1>
      <p>Visualize how different Machine Learning algorithms separate data. See where linear models fail and non-linear models succeed.</p>
    </div>

    <div class="sim-layout">
      <div class="controls">
        <div class="control-group">
          <label>Dataset</label>
          <select id="dataset-select" onchange="changeDataset()">
            <option value="linear">Linearly Separable</option>
            <option value="xor">XOR (Cross)</option>
            <option value="circles">Concentric Circles</option>
            <option value="moons">Interleaved Moons</option>
          </select>
          <p class="description" id="dataset-desc">Two distinct clusters that can be separated by a straight line.</p>
        </div>

        <div class="control-group">
          <label>Algorithm</label>
          <select id="algo-select" onchange="trainAndDraw()">
            <option value="logistic">Logistic Regression (Linear)</option>
            <option value="knn">K-Nearest Neighbors (k=3)</option>
            <option value="svm_poly">SVM (Polynomial Kernel)</option>
            <option value="rf">Random Forest (Approx)</option>
          </select>
          <p class="description" id="algo-desc">Fits a linear decision boundary. Fails on non-linear data.</p>
        </div>
        
        <button class="btn-primary" onclick="trainAndDraw()">‚ö° Train & Visualize</button>
        <button style="background: #eee; border: none; cursor: pointer; color: #333;" onclick="generateData()">üé≤ Regenerate Data</button>

        <div class="legend">
          <div class="legend-item"><div class="dot" style="background: #e74c3c"></div> Class A</div>
          <div class="legend-item"><div class="dot" style="background: #3498db"></div> Class B</div>
        </div>
      </div>

      <div class="canvas-container" id="canvas-wrap">
        <canvas id="viz-canvas"></canvas>
      </div>
    </div>
  </div>

  <!-- Explainer Section -->
  <div class="container" style="margin-top: 3rem;">
    <section class="project-card">
      <h2>Understanding Machine Learning Decision Boundaries</h2>
      
      <h3>What Are Decision Boundaries?</h3>
      <p>A decision boundary is the line, curve, or surface that separates different classes in a machine learning model. It represents the threshold where the model switches its prediction from one class to another. Understanding these boundaries helps us visualize how different algorithms approach the fundamental challenge of classification, revealing not just their predictions but the fundamental assumptions and strategies embedded in their design.</p>
      
      <h3>The Nature of Data Patterns</h3>
      <p>The datasets in this visualizer represent fundamentally different geometric challenges that machine learning algorithms encounter in practice. Linearly separable data represents the ideal scenario for linear classifiers‚Äîtwo distinct clusters that can be perfectly separated by a straight line or hyperplane in higher dimensions. This pattern appears in real-world applications like simple medical diagnoses where a single biomarker clearly distinguishes between healthy and diseased patients, or in basic financial models where creditworthiness can be determined by a linear combination of income and debt metrics.</p>
      
      <p>The XOR pattern presents a stark contrast, embodying the classic example of non-linear separability. Named after the exclusive OR logical operation, this configuration demands that the model learn something counterintuitive: that similarity in one feature doesn't guarantee the same class label. The four quadrants cannot be separated by any straight line, demonstrating why linear models fundamentally fail on certain problems regardless of how much data we provide or how carefully we tune their parameters.</p>
      
      <p>Concentric circles introduce radial separability, where one class completely surrounds another. This pattern appears commonly in medical imaging where abnormal tissue might surround normal tissue, or in geographic data where urban areas are surrounded by rural regions. The circular relationship represents a different kind of non-linearity than XOR‚Äîone based on distance from a central point rather than complex logical relationships. Linear models cannot capture this radial structure because they can only create half-spaces in the feature domain.</p>
      
      <p>The interleaved moons pattern presents perhaps the most visually striking challenge‚Äîtwo crescent shapes that interlock like puzzle pieces. This configuration mimics real-world scenarios where classes have curved boundaries and partial overlap, requiring sophisticated models to achieve good separation. The moons pattern tests whether algorithms can handle both curvature and proximity, as points from different classes can be closer to each other than to members of their own class.</p>
      
      <h3>The Linear Approach: Logistic Regression</h3>
      <p>Logistic regression represents the foundation of classification algorithms, fitting a straight line‚Äîor hyperplane in higher dimensions‚Äîthat best separates the classes using a linear combination of features. The decision boundary follows the equation w‚ÇÅx + w‚ÇÇy + b = 0, where the weights determine the orientation and the bias term sets the position. This geometric simplicity is both the model's greatest strength and its fundamental limitation.</p>
      
      <p>The appeal of logistic regression lies in its interpretability and efficiency. Each coefficient directly indicates how a feature influences the classification decision, making the model's reasoning transparent to human observers. Training is fast even on large datasets, and the model naturally provides probability estimates that quantify prediction confidence. When data is truly linearly separable, logistic regression finds the optimal boundary with mathematical elegance, making it ideal for applications like spam detection or basic risk assessment where the underlying relationships are genuinely linear.</p>
      
      <p>However, this linear assumption becomes a critical weakness when facing non-linear patterns. Logistic regression fails completely on XOR and circular patterns because no amount of training can bend a straight line into the curves these patterns require. The model must assume a linear relationship between features and the log-odds of class membership, an assumption that simply doesn't hold for many real-world phenomena. In medical diagnosis, for instance, the interaction between multiple biomarkers often produces non-linear decision boundaries that logistic regression cannot capture without manual feature engineering.</p>
      
      <h3>Local Learning: K-Nearest Neighbors</h3>
      <p>K-Nearest Neighbors takes a fundamentally different approach, eschewing global models in favor of local decision-making. The algorithm classifies each point based on the majority vote of its k nearest neighbors, creating complex, irregular decision boundaries that adapt naturally to local data patterns. This flexibility allows KNN to handle the XOR pattern, concentric circles, and interleaved moons without any explicit feature engineering or parameter tuning beyond choosing k.</p>
      
      <p>The beauty of KNN lies in its lack of assumptions about the data distribution. While logistic regression imposes a linear structure and other algorithms make various distributional assumptions, KNN simply says "you are what your neighbors are." This makes the concept intuitive and the implementation straightforward. With sufficient data, KNN can approximate arbitrarily complex decision boundaries, making it surprisingly powerful for pattern recognition tasks where the true boundary shape is unknown.</p>
      
      <p>Yet this flexibility comes with significant costs. KNN is highly sensitive to noise and irrelevant features‚Äîa single mislabeled point or an uninformative dimension can distort the neighborhood structure and degrade predictions. The algorithm becomes computationally expensive for large datasets because each prediction requires calculating distances to all training points. The choice of k involves a subtle tradeoff: small values make the model sensitive to noise, while large values smooth out genuine local patterns. Perhaps most critically, KNN suffers in high-dimensional spaces where the concept of "nearest neighbor" becomes less meaningful as all points become approximately equidistant‚Äîthe infamous curse of dimensionality. Despite these limitations, KNN remains valuable for recommendation systems, anomaly detection, and image classification where local similarity is genuinely predictive.</p>
      
      <h3>Kernel Methods: SVM with Polynomial Features</h3>
      <p>Support Vector Machines with polynomial kernels employ an elegant mathematical trick: they map data to higher dimensions using polynomial features like x¬≤, y¬≤, and xy, where linear separation becomes possible. What appears as a curved boundary in the original two-dimensional space is actually a straight hyperplane in this expanded feature space. This approach allows SVMs to handle non-linear patterns while maintaining many of the theoretical guarantees of linear methods.</p>
      
      <p>The polynomial kernel SVM can solve the concentric circles problem by recognizing that the radial pattern becomes linearly separable when we add quadratic terms. Similarly, it can handle some curved boundaries in the moons pattern, though very complex configurations might require higher-degree polynomials or different kernel functions. The model is memory efficient because it only needs to store support vectors‚Äîthe critical points near the decision boundary‚Äîrather than the entire training set. This efficiency, combined with robustness to overfitting when properly regularized, makes kernel SVMs effective in high-dimensional spaces where other methods struggle.</p>
      
      <p>However, SVMs are sensitive to feature scaling because distance calculations in the kernel depend on absolute feature magnitudes. Unlike logistic regression, standard SVMs don't provide probability estimates directly, though calibration methods can approximate them. The choice of kernel type and hyperparameters becomes crucial‚Äîa polynomial kernel might work well for circles but poorly for XOR, while a radial basis function kernel might succeed where polynomial fails. Training can also be slow on very large datasets because the optimization problem scales quadratically with the number of samples. Despite these considerations, SVMs excel in text classification, image recognition, and bioinformatics where high-dimensional non-linear patterns are common and interpretability is less critical than prediction accuracy.</p>
      
      <h3>Recursive Partitioning: Random Forests and Decision Trees</h3>
      <p>Decision trees take yet another approach, recursively splitting the feature space into rectangular regions. Each split creates a boundary parallel to a feature axis, gradually partitioning the space into smaller regions where one class dominates. A random forest extends this by building multiple trees on random subsets of data and features, then combining their predictions through voting. The resulting decision boundaries appear as complex mosaics of rectangles that can approximate curved and irregular shapes through many small axis-aligned cuts.</p>
      
      <p>This recursive partitioning strategy handles non-linear patterns naturally. Decision trees can solve XOR with just two splits‚Äîone horizontal and one vertical. They can approximate circular boundaries by creating a grid of small rectangles that collectively outline the circle. The moons pattern requires more splits but remains solvable. Beyond this flexibility, tree-based methods provide feature importance scores that indicate which variables most influence predictions, making them valuable for exploratory analysis. They're robust to outliers because splits are based on sorted feature values rather than absolute magnitudes, and they naturally handle mixed data types without requiring numerical encoding.</p>
      
      <p>The rectangular nature of tree boundaries can be both a strength and a limitation. While many small rectangles can approximate any shape, the approximation may be inefficient compared to methods that can create truly curved boundaries. Deep trees can overfit dramatically, memorizing training data by creating tiny regions around individual points. Trees also show bias toward features with more levels or higher cardinality, potentially favoring less informative variables. The rectangular boundaries work less effectively than linear methods when the true boundary is indeed linear, as many small cuts must approximate what could be captured by a single straight line. Nevertheless, decision trees and random forests have become workhorses of practical machine learning, finding applications in medical diagnosis, fraud detection, and ecological modeling where interpretability and robustness matter as much as raw accuracy.</p>
      
      <h3>Fundamental Tradeoffs and Practical Wisdom</h3>
      <p>The comparison of these algorithms reveals a fundamental tension in machine learning: the bias-variance tradeoff. Linear models like logistic regression have high bias but low variance‚Äîthey make strong assumptions about the form of the decision boundary, but these assumptions make them consistent and stable across different training sets. Non-linear models like KNN have low bias but high variance‚Äîthey're flexible enough to capture complex patterns, but this flexibility makes them unstable and sensitive to the particular quirks of the training data. The optimal choice depends critically on your data's true underlying pattern and the amount of training data available.</p>
      
      <p>This connects to the No Free Lunch theorem, a sobering mathematical result showing that no single algorithm works best for all problems when averaged across all possible data distributions. The "best" model depends on your specific dataset, noise level, sample size, and the true underlying relationship between features and classes. An algorithm that excels on concentric circles might fail on XOR, and vice versa. This reality makes algorithm selection as much art as science, requiring domain knowledge, experimentation, and careful validation.</p>
      
      <p>Practical considerations extend beyond predictive accuracy. In real applications, we must balance interpretability requirements with model complexity. A logistic regression model that you can explain to stakeholders‚Äîshowing exactly how each feature influences the decision‚Äîmight be more valuable than a random forest with slightly better accuracy but opaque reasoning. Training time matters when models need frequent retraining on new data. Prediction speed becomes critical in real-time applications where milliseconds count. The relative costs of false positives and false negatives should influence both algorithm choice and decision threshold setting. Sometimes the simple, interpretable, fast solution is genuinely the right choice, even if more sophisticated methods could eke out marginal improvements in accuracy metrics.</p>
      
      <h3>Building Intuition Through Visualization</h3>
      <p>This interactive visualizer helps develop the intuition needed for thoughtful algorithm selection. By experimenting with each dataset and algorithm combination, you can observe how linear models create straight boundaries regardless of data shape, revealing both their reliability and their rigidity. Watch how KNN creates irregular, locally-adaptive boundaries that flow around data clusters but can be disrupted by noise. See how polynomial SVM generates smooth curves that elegantly capture radial and curved patterns. Notice how decision trees partition space into rectangles that collectively approximate complex shapes through many small cuts.</p>
      
      <p>Understanding these patterns transforms algorithm selection from arbitrary choice to informed decision-making. When you encounter a new classification problem, you'll recognize whether the decision boundary is likely to be linear, curved, or highly irregular. You'll anticipate which algorithms might struggle and which might excel. You'll know when to start with a simple baseline or when the problem demands non-linear methods from the outset. This intuition, developed through hands-on experimentation and visualization, proves invaluable as you tackle increasingly complex machine learning challenges in research and practice.</p>
    </section>
  </div>

  <script>
    const canvas = document.getElementById('viz-canvas');
    const ctx = canvas.getContext('2d');
    let width, height;
    let points = [];
    let gridState = []; // Cache for decision boundary
    
    // Initialize
    function resize() {
      const wrap = document.getElementById('canvas-wrap');
      width = canvas.width = wrap.clientWidth;
      height = canvas.height = wrap.clientHeight;
      trainAndDraw();
    }
    window.addEventListener('resize', resize);
    window.addEventListener('load', () => {
      generateData();
      resize();
    });

    // Data Generation
    function generateData() {
      points = [];
      const type = document.getElementById('dataset-select').value;
      const n = 200;
      const margin = 50;
      
      // Helper random range
      const rand = (min, max) => Math.random() * (max - min) + min;
      const randG = () => (Math.random() + Math.random() + Math.random() + Math.random() + Math.random() + Math.random() - 3) / 3; // Gaussian approx

      if (type === 'linear') {
        // Two blobs
        for(let i=0; i<n/2; i++) {
          points.push({x: width*0.25 + randG()*width*0.1, y: height*0.25 + randG()*height*0.1, label: 0});
          points.push({x: width*0.75 + randG()*width*0.1, y: height*0.75 + randG()*height*0.1, label: 1});
        }
        document.getElementById('dataset-desc').innerText = "Two distinct clusters that can be separated by a straight line.";
      } 
      else if (type === 'xor') {
        // 4 quadrants
        for(let i=0; i<n; i++) {
          const x = rand(margin, width-margin);
          const y = rand(margin, height-margin);
          const label = ((x < width/2 && y < height/2) || (x > width/2 && y > height/2)) ? 0 : 1;
          points.push({x, y, label});
        }
        document.getElementById('dataset-desc').innerText = "The classic XOR problem. Requires a non-linear boundary.";
      }
      else if (type === 'circles') {
        // Concentric
        for(let i=0; i<n; i++) {
          const angle = rand(0, Math.PI*2);
          const r1 = rand(0, height*0.2);
          const r2 = rand(height*0.25, height*0.4);
          
          // Inner
          points.push({
            x: width/2 + Math.cos(angle) * r1,
            y: height/2 + Math.sin(angle) * r1,
            label: 0
          });
          // Outer
          points.push({
            x: width/2 + Math.cos(angle) * r2,
            y: height/2 + Math.sin(angle) * r2,
            label: 1
          });
        }
        document.getElementById('dataset-desc').innerText = "One class surrounds the other. Impossible for linear classifiers.";
      }
      else if (type === 'moons') {
        // Interleaved half-circles
        for(let i=0; i<n/2; i++) {
            const a = rand(0, Math.PI);
            points.push({ x: width*0.35 + Math.cos(a)*100 + randG()*10, y: height*0.5 - Math.sin(a)*100 + randG()*10, label: 0 });
            points.push({ x: width*0.65 + Math.cos(a + Math.PI)*100 + randG()*10, y: height*0.5 - Math.sin(a + Math.PI)*100 + randG()*10, label: 1 });
        }
        document.getElementById('dataset-desc').innerText = "Two interlocking shapes. Requires a complex non-linear boundary.";
      }
    }

    function changeDataset() {
      generateData();
      trainAndDraw();
    }

    // Algorithms (Simplified Implementations)
    function predict(x, y, algo) {
      if (algo === 'logistic') {
        // Simple logistic regression trained via GD
        // We'll use the model stored in global state 'linearModel'
        const z = linearModel.w1 * x + linearModel.w2 * y + linearModel.b;
        return z > 0 ? 1 : 0;
      }
      else if (algo === 'knn') {
        // 3-NN
        const k = 3;
        // Find distances
        const dists = points.map(p => ({
          d: (p.x - x)**2 + (p.y - y)**2,
          label: p.label
        }));
        dists.sort((a, b) => a.d - b.d);
        // Vote
        let vote = 0;
        for(let i=0; i<k; i++) vote += (dists[i].label === 1 ? 1 : -1);
        return vote > 0 ? 1 : 0;
      }
      else if (algo === 'svm_poly') {
        // Polynomial Feature expansion (x, y, x^2, y^2, xy) + Linear
        // Using the trained 'polyModel'
        const x_n = (x - width/2) / 100; // Normalize roughly
        const y_n = (y - height/2) / 100;
        const z = polyModel.w[0]*x_n + polyModel.w[1]*y_n + 
                  polyModel.w[2]*x_n*x_n + polyModel.w[3]*y_n*y_n + 
                  polyModel.w[4]*x_n*y_n + polyModel.b;
        return z > 0 ? 1 : 0;
      }
      else if (algo === 'rf') {
         // Random Forest approximation (Rectangular regions)
         // We'll just use 1-NN which looks like RF/Voronoi for dense data, 
         // or actually implement a simple decision tree logic?
         // Let's simulate a Decision Tree (recursive partitioning)
         return predictTree(treeModel, x, y);
      }
      return 0;
    }

    // Global Models
    let linearModel = { w1: 0, w2: 0, b: 0 };
    let polyModel = { w: [0,0,0,0,0], b: 0 };
    let treeModel = null;

    function train(algo) {
      const desc = document.getElementById('algo-desc');
      if (algo === 'logistic') {
        desc.innerText = "Fits a straight line (hyperplane). Fails completely on Circles and XOR.";
        // Train Logic (Gradient Descent)
        let w1 = Math.random() - 0.5, w2 = Math.random() - 0.5, b = 0;
        const lr = 0.01; // Learning rate
        
        // Normalize data for training
        const normPoints = points.map(p => ({
            x: (p.x - width/2)/100,
            y: (p.y - height/2)/100,
            l: p.label
        }));

        for(let iter=0; iter<1000; iter++) {
            for(let p of normPoints) {
                const z = w1*p.x + w2*p.y + b;
                const pred = 1 / (1 + Math.exp(-z));
                const err = pred - p.l;
                w1 -= lr * err * p.x;
                w2 -= lr * err * p.y;
                b -= lr * err;
            }
        }
        linearModel = { w1, w2, b };
      }
      else if (algo === 'knn') {
        desc.innerText = "K-Nearest Neighbors. Classifies based on local neighborhood. Adapts well to complex shapes but sensitive to noise.";
        // No training needed, lazy learning
      }
      else if (algo === 'svm_poly') {
        desc.innerText = "SVM with Polynomial features (x¬≤, y¬≤, xy). Can solve Circles and some curves, but maybe not complex XOR/Spirals.";
         // Train Logic (Gradient Descent on Poly Features)
        let w = [0,0,0,0,0].map(() => Math.random()-0.5);
        let b = 0;
        const lr = 0.01;
        
        const normPoints = points.map(p => {
            const x = (p.x - width/2)/100;
            const y = (p.y - height/2)/100;
            return { f: [x, y, x*x, y*y, x*y], l: p.label };
        });

        for(let iter=0; iter<2000; iter++) {
            for(let p of normPoints) {
                let z = b;
                for(let i=0; i<5; i++) z += w[i]*p.f[i];
                const pred = 1 / (1 + Math.exp(-z)); // Using sigmoid as proxy for SVM soft margin
                const err = pred - p.l;
                for(let i=0; i<5; i++) w[i] -= lr * err * p.f[i];
                b -= lr * err;
            }
        }
        polyModel = { w, b };
      }
      else if (algo === 'rf') {
        desc.innerText = "Decision Tree/Random Forest style. Cuts space into rectangles. Handles non-linear data well.";
        // Build a simple Decision Tree
        treeModel = buildTree(points, 0, 5);
      }
    }

    // Simple Decision Tree Construction
    function buildTree(subset, depth, maxDepth) {
        // Base cases
        if (depth >= maxDepth || subset.length < 5) {
            const c0 = subset.filter(p=>p.label===0).length;
            const c1 = subset.filter(p=>p.label===1).length;
            return { leaf: true, val: c1 > c0 ? 1 : 0 };
        }
        
        // Find best split (random search for simplicity in viz)
        let bestSplit = null;
        let bestImpurity = 1; // Gini

        for(let i=0; i<20; i++) {
            const axis = Math.random() > 0.5 ? 'x' : 'y';
            // Pick a random split point from data range
            const vals = subset.map(p => p[axis]);
            const min = Math.min(...vals);
            const max = Math.max(...vals);
            const splitVal = min + Math.random()*(max-min);

            const left = subset.filter(p => p[axis] < splitVal);
            const right = subset.filter(p => p[axis] >= splitVal);

            if(left.length === 0 || right.length === 0) continue;

            // Gini
            const gini = (s) => {
                if(s.length===0) return 0;
                const p = s.filter(x=>x.label===1).length / s.length;
                return 2 * p * (1-p);
            };
            const impurity = (left.length*gini(left) + right.length*gini(right)) / subset.length;

            if (impurity < bestImpurity) {
                bestImpurity = impurity;
                bestSplit = { axis, val: splitVal, left, right };
            }
        }

        if (!bestSplit) return { leaf: true, val: subset.filter(p=>p.label===1).length > subset.length/2 ? 1 : 0 };

        return {
            leaf: false,
            axis: bestSplit.axis,
            splitVal: bestSplit.val,
            left: buildTree(bestSplit.left, depth+1, maxDepth),
            right: buildTree(bestSplit.right, depth+1, maxDepth)
        };
    }

    function predictTree(node, x, y) {
        if (node.leaf) return node.val;
        const val = node.axis === 'x' ? x : y;
        return val < node.splitVal ? predictTree(node.left, x, y) : predictTree(node.right, x, y);
    }

    function drawLinearBoundary() {
      // Draw the actual linear decision boundary line
      const { w1, w2, b } = linearModel;
      if (Math.abs(w1) < 0.001 && Math.abs(w2) < 0.001) return; // Skip if no meaningful weights
      
      ctx.strokeStyle = '#34495e';
      ctx.lineWidth = 3;
      ctx.setLineDash([5, 5]);
      ctx.beginPath();
      
      // Calculate line endpoints: w1*x + w2*y + b = 0 => y = -(w1*x + b)/w2
      if (Math.abs(w2) > 0.001) {
        // Draw horizontal line across canvas
        const x1 = 0, x2 = width;
        const y1 = -(w1*(x1-width/2)/100 + b) * 100 + height/2;
        const y2 = -(w1*(x2-width/2)/100 + b) * 100 + height/2;
        ctx.moveTo(x1, y1);
        ctx.lineTo(x2, y2);
      } else {
        // Vertical line
        const x = -b * 100 / w1 + width/2;
        ctx.moveTo(x, 0);
        ctx.lineTo(x, height);
      }
      
      ctx.stroke();
      ctx.setLineDash([]);
    }


    function trainAndDraw() {
      const algo = document.getElementById('algo-select').value;
      train(algo);
      
      // Draw Background (Decision Boundary)
      // Use a coarse grid for speed, then upscale
      const resolution = 6; // Evaluate every 6th pixel for better boundary visibility
      const cols = Math.ceil(width / resolution);
      const rows = Math.ceil(height / resolution);
      
      ctx.clearRect(0, 0, width, height);
      
      // Normalize coordinate system in predict function to match train
      const normalize = (v, size) => (v - size/2)/100;

      for (let i = 0; i < cols; i++) {
        for (let j = 0; j < rows; j++) {
          const x = i * resolution;
          const y = j * resolution;
          const p = predict(x, y, algo);
          
          ctx.fillStyle = p === 0 ? 'rgba(231, 76, 60, 0.15)' : 'rgba(52, 152, 219, 0.15)';
          ctx.fillRect(x, y, resolution, resolution);
        }
      }
      
      // Draw decision boundary line for linear methods
      if (algo === 'logistic') {
        drawLinearBoundary();
      }
      
      // Draw Points
      for(let p of points) {
        ctx.beginPath();
        ctx.arc(p.x, p.y, 5, 0, Math.PI*2);
        ctx.fillStyle = p.label === 0 ? '#e74c3c' : '#3498db'; // Red vs Blue
        ctx.strokeStyle = 'white';
        ctx.lineWidth = 2;
        ctx.fill();
        ctx.stroke();
      }
    }

    // Particle JS Init
    document.addEventListener("DOMContentLoaded", () => {
      if(window.particlesJS) {
          particlesJS('particles-js', {
            particles: {
              number: { value: 30 },
              color: { value: '#aaa' },
              shape: { type: 'circle' },
              opacity: { value: 0.2 },
              size: { value: 3 },
              move: { enable: true, speed: 1 }
            }
          });
      }
    });
  </script>
</body>
</html>

